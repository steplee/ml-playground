import numpy as np
import sys
import matplotlib.pyplot as plt

# Data
from tensorflow.examples.tutorials.mnist import input_data

# Utility function, list of [H,W,C] -> plotted image
def mk_grid(imgs,sz=None,pad=5, width=None, cmap=None, interp='none'):
    n = len(imgs)
    if width == None:
        width = int(np.sqrt(n)+1)
    if sz==None:
        sz = (  max([i.shape[0] for i in imgs]),
                max([i.shape[1] for i in imgs]),
                max([i.shape[2] for i in imgs]))

    z = np.zeros([(sz[0]+pad)*(width), (sz[1]+pad)*(width), sz[2]])

    _x,_y = 0,0
    for j,im in enumerate(imgs):
        z[_y:_y+im.shape[0], _x:_x+im.shape[1], ...] = im[...]
        _x += sz[1] + pad
        if (1+j) % (width) == 0:
            _x = 0
            _y += sz[0] + pad

    if z.shape[-1] == 1:
        z = z[...,0] # pyplot doesn't like 1-dim channel
        cmap = cmap if cmap else 'gray'

    plt.imshow(z, cmap=cmap, interpolation=interp)
    plt.show()

try:
    __mnist
except:
    __mnist = input_data.read_data_sets("/data/mnist/", one_hot=True)



# Additive coupling layers
# x is batched vector
# :m is forward function
# :mode is either `forward` or `reverse`
def additive_couple(x, m, mode='forward'):
    B = x.shape[0] # batch size
    D = x.shape[1] # dimensionality
    d = x.shape[1]//2

    if mode == 'forward':
        y1 = x[:,0:d]
        y2 = x[:,d:] + m(y1)
    else:
        y1 = x[:,0:d]
        y2 = x[:,d:] - m(y1)

    #print(y1,y2)

    y = np.hstack([y1,y2])

    Sii = np.ones(B) * 1.0
    y = y.T
    y *= Sii
    y = y.T

    return y, Sii


m1 = np.square

log2pi = np.log(np.pi*2.0)

def nice(batch_size=10):

    mnist = __mnist
    batches = 600
    

    D = 784
    theta = np.abs(np.random.normal(size=D//2))
    theta2 = np.abs(np.random.normal(size=D//2))
    def my_m(x):
        return x * theta
    def my_m2(x):
        return x * theta2

    mix_inds = list(range(D//2,D))+list(range(D//2))

    for batch in range(batches):
        bx,by = mnist.train.next_batch(batch_size)
        for epoch in range(3):

            #m = np.square
            m = my_m
            m2 = my_m2

            # Map to z-space
            fx,det = additive_couple(bx, m, mode="forward")
            # swap and redo
            bx2 = fx[:,mix_inds]
            fx2,det = additive_couple(bx2, m2, mode="forward")

            # Eval prior & l
            logdet = np.log(np.abs(det))
            #print('det',logdet)
            px = (-1/2) * np.sum(fx2, axis=1) + log2pi*(-D/2)

            # Use px as loss and back-prop
            lr = .00003
            dph = -2 * fx2
            theta2 -= lr*np.sum(bx2[:,:D//2],axis=0)
            theta -= lr*theta2*np.sum(bx[:,:D//2],axis=0)

            #theta = theta*(1-lr*.01) - lr*np.sum(bx[:,:D//2],axis=0)


            print(np.mean(px))
            #print("weights",np.abs(theta).mean())

    # Map back to x-space, then plot
    fx_corrupt = fx2 + np.random.normal(size=fx.shape) * .01
    #fx_corrupt = fx2 
    fx_inv,det2 = additive_couple(fx_corrupt, m2, mode="backward")
    fx_inv = fx_inv[:,mix_inds]
    fx_inv,det2 = additive_couple(fx_inv, m, mode="backward")
    p_bx = bx.clip(0,1).reshape([batch_size, 28,28, 1])
    p_fx_inv = fx_inv.clip(0,1).reshape([batch_size, 28,28, 1])
    mk_grid(list(p_bx) + list(p_fx_inv))


    # Do we do well on fake data?
    bx_fake = (bx*(.5+np.random.normal(size=bx.shape)) * 2.0).clip(0,1)
    fx,det = additive_couple(bx_fake, m, mode="forward")
    bx_fake2 = fx[:,mix_inds]
    fx2,det = additive_couple(bx_fake2, m2, mode="forward")
    px = (-1/2) * np.sum(fx2, axis=1) + log2pi*(-D/2)
    print("Fake data score: {}".format(px.mean()))



def init():
    pass

